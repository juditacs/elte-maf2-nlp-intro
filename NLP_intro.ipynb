{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Natural Language Processing\n",
    "\n",
    "November 18, 2021\n",
    "\n",
    "[Judit Ács](https://hlt.bme.hu/en/judit),\n",
    "[Ádám Kovács](https://hlt.bme.hu/en/kovacsadam)\n",
    "\n",
    "May 17, 2023\n",
    "\n",
    "[Dávid Márk Nemeskey](https://hlt.bme.hu/en/david)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Agenda\n",
    "\n",
    "- Overview of NLP tasks\n",
    "- SpaCy NLP toolkit\n",
    "- Neural networks in NLP, sequence modeling\n",
    "- Pre-trained language models, BERT\n",
    "- optional: BERT usage examples\n",
    "- optional: a full sequence classification example with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need NLP?\n",
    "\n",
    "- Make the computer understand text\n",
    "- Extract useful information from it\n",
    "- A collection that helps us processing huge amount of texts\n",
    "- We have two directions:\n",
    "    - Analysis: Convert text to a structural representation\n",
    "    - Generation: Generate text from formal representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NLP tasks\n",
    "\n",
    "### End-user tasks\n",
    "\n",
    "- Spellchecking\n",
    "- Machine translation\n",
    "- Chatbots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### \"Real\" tasks?\n",
    "\n",
    "- Low level NLP tasks:\n",
    "    - tokenization\n",
    "    - lemmatization\n",
    "    - POS tagging\n",
    "    - syntactic parsing\n",
    "    - dependency parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- High level or downstream tasks:\n",
    "    - summarization\n",
    "    - question answering\n",
    "    - information extraction (e.g. NER tagging)\n",
    "    - relation extraction \n",
    "    - chatbots\n",
    "    - machine translation\n",
    "    - etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Spacy](https://spacy.io)\n",
    "\n",
    "- Open-source NLP library for Python\n",
    "- For demonstrating NLP tasks, we are going to use the library [spacy](https://spacy.io/) a lot.\n",
    "- It features a lot of out-of-the-box models for NLP\n",
    "- NER, POS tagging, dependency parsing, vectorization\n",
    "- Hosts models for many languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "# Loading the english model\n",
    "    nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "- Splitting text into words, sentences, documents, etc..\n",
    "- One of the goals of tokenizing text into words is to create a <strong>vocabulary</strong>\n",
    "\n",
    "<p><em>Muffins cost <strong>$3.88</strong> in New York. Please buy me two as I <strong>can't</strong> go. <strong>They'll</strong> taste good. I'm going to <strong>Finland's</strong> capital to hear about <strong>state-of-the-art</strong> solutions in NLP.</em></p>\n",
    "\n",
    "- $3.88 - split on the period?\n",
    "- can't - can not?\n",
    "- They'll - they will?\n",
    "- Finland's - Finland?\n",
    "- state-of-the-art?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sens = \"Muffins cost $3.88 in New York. Please buy me two as I can't go.\" \\\n",
    "\" They'll taste good. I'm going to Finland's capital to hear about state-of-the-art solutions in NLP.\"\n",
    "\n",
    "print(sens.split())\n",
    "print(len(sens.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sens = \"Muffins cost $3.88 in New York. Please buy me two as I can't go.\" \\\n",
    "\" They'll taste good. I'm going to Finland's capital to hear about state-of-the-art solutions in NLP.\"\n",
    "\n",
    "doc = nlp(sens)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for sen in doc.sents:\n",
    "    print(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc[:5]:\n",
    "    print(f\"{token.text=}, {token.is_alpha=}, {token.is_stop=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    {'text': token.text, 'is_alpha': token.is_alpha, 'is_stop': token.is_stop}\n",
    "    for token in doc\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lemmatization\n",
    "\n",
    "- The goal of lemmatization is to find the dictionary form of the words\n",
    "- Called the \"lemma\" of a word\n",
    "- _dogs_ -> _dog_ , _went_ -> _go_\n",
    "- Ambiguity plays a role: _saw_ -> _see_?\n",
    "- Needs POS tag to disambiguate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"I saw two dogs yesterday.\")\n",
    "\n",
    "lemmata = [token.lemma_ for token in doc]\n",
    "print(lemmata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Stemming\n",
    "\n",
    "- Similar to lemmatization, it tries to normalize the text\n",
    "- Stems are always substrings of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### POS tagging\n",
    "\n",
    "- Words can be groupped into grammatical categories.\n",
    "- These are called the Part Of Speech tags of the words.\n",
    "- Words belonging to the same group are interchangable\n",
    "- Ambiguity: _guard_ ?\n",
    "- Similar to _szófaj_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(\"The white dog went to play football yesterday.\")\n",
    "\n",
    "[token.pos_ for token in doc]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h3 id=\"Morphological-analysis\">Morphological analysis</h3>\n",
    "<ul>\n",
    "<li>Splitting words into morphemes</li>\n",
    "<li>Morphemes are the smallest meaningful units in a language (part of the words)</li>\n",
    "<li>friend<span style=\"color: #e03e2d;\">s</span>, wait<span style=\"color: #e03e2d;\">ing</span>, friend<span style=\"color: #e03e2d;\">li</span><span style=\"color: #3598db;\">er</span></li>\n",
    "<li>Tagging them with morphological tags</li>\n",
    "<li>Ambiguity: <em>v&aacute;rnak</em></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Named entity recognition\n",
    "\n",
    "- Identify the present entities in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sens = \"Muffins cost $3.88 in New York. Please buy me two as I can't go.\" \\\n",
    "\" They'll taste good. I'm going to Finland's capital to hear about state-of-the-art solutions in NLP.\"\n",
    "\n",
    "doc = nlp(sens)\n",
    "for ent in doc.ents:\n",
    "    print(ent)\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Language modelling\n",
    "\n",
    "- One of the most important task in NLP\n",
    "- The goal is to compute the \"probability\" of a sentence\n",
    "- Can be used in:\n",
    "    - Machine Translation\n",
    "    - Text generation\n",
    "    - Correcting spelling\n",
    "    - Word vectors?\n",
    "- P(the quick brown __fox__) > P(the quick brown __stick__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### <center>Lexical Inference, Natural Language Inference</center>\n",
    "\n",
    "\n",
    "\n",
    "<div class=\"frame\">\n",
    "\n",
    "| **entailment**                                                |     |     |\n",
    "|:--------------------------------------------------------------|:----|:----|\n",
    "| A young family enjoys feeling ocean waves lap at their feet.  |     |     |\n",
    "| A family is at the beach                                      |     |     |\n",
    "| **contradiction**                                             |     |     |\n",
    "| There is no man wearing a black helmet and pushing a bicycle  |     |     |\n",
    "| One man is wearing a black helmet and pushing a bicycle       |     |     |\n",
    "| **neutral**                                                   |     |     |\n",
    "| An old man with a package poses in front of an advertisement. |     |     |\n",
    "| A man poses in front of an ad for beer.                       |     |     |\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Demos\n",
    "\n",
    "- http://e-magyar.hu/hu/parser\n",
    "- https://demo.allennlp.org/\n",
    "- https://talktotransformer.com/\n",
    "- [GPT-3](https://github.com/elyase/awesome-gpt3) (*has 175B parameters*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence modeling\n",
    "\n",
    "## Recurrent neural networks\n",
    "\n",
    "- In NLP, recurrent neural networks (RNN) are commonly used to analyse sequences. \n",
    "- It takes in a sequence of words, one at a time, and produces hidden states ($h$) after each steps. \n",
    "- RNN-s are used recurrently by feeding in the current word and the hidden state from the previous word.\n",
    "- Once we have our final hidden state, $h_T$, (from feeding in the last word in the sequence, $x_T$) we feed it through a linear layer, $f$ (fully connected layer) to reduce the dimension into the dimension of the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "    h_t = \\sigma_h(W_{h} x_t + U_{h} h_{t-1} + b_h) \\\\\n",
    "    y_t = \\sigma_y(W_{y} h_t + b_y),\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $x_t$ is the input at time $t$\n",
    "* $y_t$ is the output at time $t$\n",
    "* $h_t$ is the hidden state at time $t$,\n",
    "* $W_h, W_y, U_h, b_h, b_y$ are the RNN's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment1.png)\n",
    "\n",
    "_(image from bentrevett)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM\n",
    "\n",
    "- One of the biggest problem of recurrent neural networks is the vanishing gradient problem. \n",
    "- It happens when the gradient shrinks during bakcpropagarion. \n",
    "- If it becomes very small, the network stops learning. This mostly happen when long sentences are present. \n",
    "- LSTM networks address this problem by having an inner memory cell to remember important information or forget others. \n",
    "- LSTM has a similar flow as a RNN, it processes data and passes information as it propagates forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "f_t = \\sigma_g(W_{f} x_t + U_{f} h_{t-1} + b_f) \\\\\n",
    "i_t = \\sigma_g(W_{i} x_t + U_{i} h_{t-1} + b_i) \\\\\n",
    "o_t = \\sigma_g(W_{o} x_t + U_{o} h_{t-1} + b_o) \\\\\n",
    "\\tilde{c}_t = \\sigma_c(W_{c} x_t + U_{c} h_{t-1} + b_c) \\\\\n",
    "c_t = f_t \\circ c_{t-1} + i_t \\circ \\tilde{c}_t \\\\\n",
    "h_t = o_t \\circ \\sigma_h(c_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequence elements\n",
    "\n",
    "We deal with sequences in NLP:\n",
    "- a token is a sequence of characters/morphemes\n",
    "- a sentence is a sequence of tokens\n",
    "- a paragraph is a sequence of sentences\n",
    "- a dialogue is a sequence of utterances\n",
    "- etc.\n",
    "\n",
    "What are the elements of these sequences?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Words\n",
    "\n",
    "Pros:\n",
    "\n",
    "- More or less well-defined in most languages\n",
    "- Relatively short sequences (a sentence is rarely longer than 30 tokens)\n",
    "\n",
    "Cons:\n",
    "- Difficult tokenization in some languages\n",
    "- Large vocabulary (100,000+ easily)\n",
    "- Out-of-vocabulary words are always there regardless of the size of the vocabulary\n",
    "- Many rare words\n",
    "    - Hapax: a word that only appears once in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Characters\n",
    "\n",
    "Pros:\n",
    "- Smaller vocabulary although logographic writing systems (Chinese and Japanese) have thousands of characters\n",
    "- Easy tokenization\n",
    "- Well defined: Unicode symbols\n",
    "\n",
    "Cons:\n",
    "- Long sequences\n",
    "- Too fine-grained, token level information is lost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Subwords\n",
    "\n",
    "- Multiple characters but smaller than words\n",
    "- Modern language models use subword vocabularies\n",
    "- We will cover these next week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Embeddings\n",
    "\n",
    "## Word embeddings\n",
    "\n",
    "- map each word to a small dimensional (around 100-300) continuous vector\n",
    "- similar words should have similar vectors\n",
    "    - Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Creating word embeddings\n",
    "\n",
    "Word embeddings are learned with neural networks. The target can be:\n",
    "\n",
    "- predict the word given the context - The Continous Bag Of Words model (CBOW)\n",
    "- predict the context given a words - The SkipGram model\n",
    "\n",
    "The training examples are generated from big text corpora.\n",
    "- no need for expensive manual annotation\n",
    "- only limited by the availability of textual data\n",
    "\n",
    "For example from the sentence “The quick brown fox jumps over the lazy dog.” we can generate the following inputs:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![training examples](http://mccormickml.com/assets/word2vec/training_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Famous static word embeddings for English\n",
    "\n",
    "- [Word2vec](https://arxiv.org/pdf/1301.3781.pdf)\n",
    "- [GLOVE](https://nlp.stanford.edu/projects/glove/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Types of sequence tasks\n",
    "\n",
    "## Sequence classification\n",
    "\n",
    "Assign a single label to the full sequence:\n",
    "\n",
    "<img src=\"img/tikz/abstract_sequence_classification.png\" width=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Applications__\n",
    "\n",
    "- Topic classification \n",
    "- Sentiment analysis: is this sentence or paragraph a positive (1) or a negative (0) review?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/tikz/example_sequence_classification.png\" width=\"500\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequence tagging\n",
    "\n",
    "Assign a label to each element of the sequence:\n",
    "\n",
    "<img src=\"img/tikz/abstract_sequence_tagging.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Applications__\n",
    "\n",
    "- part-of-speech tagging\n",
    "- named entity recognition (NER)\n",
    "\n",
    "<img src=\"img/tikz/example_sequence_tagging.png\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Seq2seq\n",
    "\n",
    "<img src=\"img/tikz/abstract_seq2seq.png\" width=600px>\n",
    "\n",
    "- Maps a source sequence to a target sequence\n",
    "    - Arbitrary length\n",
    "    \n",
    "- Two steps:\n",
    "    1. Encode: create a representation of the source\n",
    "    2. Decode: generate the target representation\n",
    "        - autoregressive: generate tokens from left-to-right one-by-one (condition on the left context)\n",
    "        \n",
    "- Applications:\n",
    "    - Neural machine translation\n",
    "    - Morphological inflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Usually implemented as two separate neural networks for example:\n",
    "    - The encoder is a bidirectional LSTM\n",
    "    - The decoder is a unidirectional LSTM\n",
    "    \n",
    "- Problems:\n",
    "    - The input sequence is compressed into a single vector\n",
    "    - The decoding steps rely on the same input representation in every step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Attention mechanism\n",
    "\n",
    "Attention:\n",
    "- emphasizes the important part of the input\n",
    "- and de-emphasizes the rest.\n",
    "- Mimics cognitive attention.\n",
    "\n",
    "Method:\n",
    "- It does this by assigning weights to the elements of the input sequence.\n",
    "- The weights depend on the current context in the decoder:\n",
    "    - the current decoder hidden state,\n",
    "    - the previous output.\n",
    "- The source vectors are multiplied by the weights and then summed -> **context vector**\n",
    "- The context vector is used for predicting the next output symbol.\n",
    "\n",
    "[image source](https://aihub.cloud.google.com/u/0/p/products%2F024b89fd-9bc8-4c24-b8a8-e347479f3270):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"img/dl/attention_mechanism.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Problems\n",
    "\n",
    "Recall that we used recurrent neural cells, specifically LSTMs to encode and decode sequences.\n",
    "\n",
    "__Problem 1. No parallelism__\n",
    "\n",
    "LSTMs are recurrent, they rely on their left and right history (horizontal arrows), so the symbols need to be processed in order -> no parallelism.\n",
    "\n",
    "__Problem 2. Long-range dependencies__\n",
    "\n",
    "Long-range dependencies are not infrequent in NLP.\n",
    "\n",
    "\"The **people/person** who called and wanted to rent your house when you go away next year **are/is** from California\" -- Miller & Chomsky 1963\n",
    "\n",
    "LSTMs have a problem capturing these because there are too many backpropagation steps between the symbols."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transformers\n",
    "\n",
    "Introduced in [Attention Is All You Need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) by Vaswani et al., 2017 (30000 citations)\n",
    "\n",
    "Transformers solve Problem 1 by relying purely on attention instead of recurrence.\n",
    "\n",
    "Not having recurrent connections means that sequence position no longer matters.\n",
    "\n",
    "Recurrence is replaced by **self attention**.\n",
    "\n",
    "Each symbol is encoded the following way:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Step 1__: the encoder 'looks' at the other symbols in the input sequence\n",
    "    - In the example above: the representation of **are/is** depends on **people/person** more than any other word in the sentence, it should receive the highest attention weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"http://jalammar.github.io/images/t/transformer_self-attention_visualization.png\", embed=True)  # from Illustrated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__Step 2__: the context vector is passed through a feed-forward network which is shared across all symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"http://jalammar.github.io/images/t/encoder_with_tensors.png\", embed=True)  # from Illustrated Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This visualization is available in the [Tensor2tensor notebook in Google Colab](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Other components\n",
    "\n",
    "__Residual connections__\n",
    "\n",
    "- Also called __skip connections__\n",
    "- The output of a module is added to the input\n",
    "\n",
    "$$\n",
    "\\text{output} = \\text{layer}(\\text{input}) + \\text{input}\n",
    "$$\n",
    "\n",
    "__Softmax__\n",
    "\n",
    "- Only used in the decoder\n",
    "- Maps the output vector to a probability distribution\n",
    "    - In other words it tells us how likely each symbol is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Multiple heads and layers\n",
    "\n",
    "Transformers have a number of additional components summarized in this figure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"img/dl/transformer.png\")  # from Vaswani et al. 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Positional encoding\n",
    "\n",
    "Without recurrence word order information is lost.\n",
    "\n",
    "Positional information is important:\n",
    "\n",
    "    John loves Mary.\n",
    "    Mary loves John.\n",
    "\n",
    "Transformers apply positional encoding:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{\\text{pos},2i} = \\sin(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}), \\\\\n",
    "\\text{PE}_{\\text{pos},2i+1} = \\cos(\\frac{\\text{pos}}{10000^{2i/d_{\\text{model}}}}),\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $d_{\\text{model}}$ is the input dimension to the Transformer, usually the embedding size\n",
    "- $\\text{pos}$ is the position of the symbol in the input sequence i.e. first word, second word etc.\n",
    "- $i$ is the coordinate index in the input vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Contextualized embeddings\n",
    "\n",
    "- static representations (GloVe, Word2vec)\n",
    "    - the same vector is assigned to each occurrence of a word\n",
    "    \n",
    "But words can have different meaning in different contexts, e.g. the word 'stick':\n",
    "\n",
    "1. Find some dry <b>sticks</b> and we'll make a campfire.\n",
    "2. Let's <b>stick</b> with glove embeddings.\n",
    "\n",
    "Contextualized embeddings take the full sentence as their input.\n",
    "    \n",
    "![elmo](http://jalammar.github.io/images/elmo-embedding-robin-williams.png)\n",
    "\n",
    "_(Peters et. al., 2018 in the ELMo paper)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ELMo\n",
    "\n",
    "**E**mbeddings from **L**anguage **Mo**dels\n",
    "\n",
    "Word representations are functions of the full sentences instead of the word alone.\n",
    "\n",
    "Two bidirectional LSTM layers are linearly combined.\n",
    "\n",
    "[Deep contextualized word representations](https://arxiv.org/abs/1802.05365) by Peters et al., 2018, 8400 citations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BERT\n",
    "\n",
    "[BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://www.aclweb.org/anthology/N19-1423/)\n",
    "by Devlin et al. 2018, 29600 citations\n",
    "\n",
    "[BERTology](https://huggingface.co/transformers/bertology.html) is the nickname for the growing amount of BERT-related research.\n",
    "\n",
    "Trained on two tasks:\n",
    "\n",
    "1. Masked language model:\n",
    "\n",
    "    1. 15% of the <s>tokens</s>wordpieces are selected at the beginning.\n",
    "    2. 80% of those are replaced with `[MASK]`,\n",
    "    3. 10% are replaced with a random token,\n",
    "    4. 10% are kept intact.\n",
    "    \n",
    "2. Next sentence prediction:\n",
    "    - Are sentences A and B consecutive sentences?\n",
    "    - Generate 50-50%.\n",
    "    - Binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"img/dl/bert_embedding.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Transformer layers\n",
    "\n",
    "\n",
    "## Finetuning\n",
    "\n",
    "1. Take a pre-trained BERT model.\n",
    "2. Add a small classification layer on top (typically a 2-layer MLP).\n",
    "3. Train BERT along with the classification layer on an annotated dataset.\n",
    "    - Much smaller than the data BERT was trained on\n",
    "\n",
    "Another option: freeze BERT and train the classification layer only.\n",
    "- Easier training regime.\n",
    "- Smaller memory footprint.\n",
    "- Worse performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "Image(\"img/dl/bert_encoding_finetuning.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## BERT pretrained checkpoints\n",
    "\n",
    "### BERT-Base\n",
    "\n",
    "- 12 layers\n",
    "- 110M parameters\n",
    "\n",
    "### BERT-Large\n",
    "\n",
    "- 24 layers\n",
    "- 340M parameters\n",
    "\n",
    "### Cased and uncased\n",
    "\n",
    "Uncased: everything is lowercased. Diacritics are removed.\n",
    "\n",
    "### Multilingual BERT - mBERT\n",
    "\n",
    "104 language version trained on the 100 largest Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BERT tokenization\n",
    "\n",
    "## WordPiece tokenizer\n",
    "\n",
    "BERT's input **must** be tokenized with BERT's own tokenizer.\n",
    "\n",
    "A middle ground between word and character tokenization.\n",
    "\n",
    "Static vocabulary:\n",
    "- Byte-pair encoding: simple frequency-based tokenization method\n",
    "- Continuation symbols (\\#\\#symbol)\n",
    "- Special tokens: `[CLS]`, `[SEP]`, `[MASK]`, `[UNK]`\n",
    "- It tokenizes everything, falling back to characters and `[UNK]` if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "`AutoTokenizer` is a factory class for pretrained tokenizers. ng id. `from_pretrained` instantiates the corresponding class and loads the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "print(len(t.get_vocab()))\n",
    "\n",
    "t.tokenize(\"My beagle's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t.tokenize(\"Русский\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Cased** models keep diacritics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "print(len(t.get_vocab()))\n",
    "\n",
    "t.tokenize(\"My beagle's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "It character tokenizes Chinese and Japanese but doesn't know all the characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t.tokenize(\"日本語\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Korean is missing from this version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t.tokenize(\"한 한국어\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## mBERT tokenization\n",
    "\n",
    "104 languages, 1 vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "len(t.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "t.tokenize(\"My puppy's name is Tündérke.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t.tokenize(\"한 한국어\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "t.tokenize(\"日本語\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Using BERT\n",
    "\n",
    "## Using `BertModel` directly\n",
    "\n",
    "`AutoModel`\n",
    "- each pretrained checkpoint has a string id. `from_pretrained` instantiates the corresponding class and loads the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')\n",
    "model = AutoModel.from_pretrained('bert-base-cased')\n",
    "type(model), type(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sentence = \"There are black cats and black dogs.\"\n",
    "\n",
    "output = model(**tokenizer(sentence, return_tensors='pt'), return_dict=True)\n",
    "\n",
    "for k, v in output.items():\n",
    "    print(f\"{k}: {v.size()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## BERT applications\n",
    "\n",
    "### Sequence classification\n",
    "\n",
    "Pretrained model for sentiment analysis.\n",
    "\n",
    "Base model: `distilbert-base-uncased`\n",
    "\n",
    "Finetuned on the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) or SST-2, a popular sentiment analysis dataset.\n",
    "\n",
    "Model id: `distilbert-base-uncased-finetuned-sst-2-english`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nlp = pipeline(\"sentiment-analysis\")\n",
    "nlp(\"This is an amazing class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp(\"This is not a good class but it's not too bad either.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp(\"This is not a class.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "del nlp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sequence tagging/labeling: Named entity recognition\n",
    "\n",
    "Base model: `bert-large-cased`\n",
    "\n",
    "Finetuned on [CoNLL-2003 NER](https://www.clips.uantwerpen.be/conll2003/ner/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "result = nlp(\"Jupiter is a planet that orbits around James the center of the Universe\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "result = nlp(\"George Clooney has a pet pig named Estella.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "del nlp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Machine translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp = pipeline(\"translation_en_to_fr\")\n",
    "print(nlp(\"Hugging Face is a technology company based in New York and Paris\", max_length=40))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Even the [blessé - blessed false cognate](https://frenchtogether.com/french-english-false-friends/) is handled correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "nlp(\"I was blessed by God after I injured my head.\", max_length=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "del nlp\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "summarizer = pipeline(\"summarization\")\n",
    "summarizer(\"Deep learning is used almost exclusively in a Linux environment.\\\n",
    "You need to be comfortable using the command line if you are serious about deep learning and NLP.\\\n",
    "    Most NLP and deep learning libraries have better support for Linux and MacOS than Windows. \\\n",
    "    Most papers nowadays release the source code for their experiments with Linux support only.\",\n",
    "           min_length=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sentiment Analysis\n",
    "- In the simplest case, decide whether a text is negative or positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sentiment = pipeline(\"sentiment-analysis\")\n",
    "sentiment(['This class is really cool! I would recommend this to anyone!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Question Answering\n",
    "\n",
    "- Given a context and a question choose the right answer\n",
    "- Can be extractive or abstractive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "question_answerer = pipeline('question-answering')\n",
    "question_answerer({\n",
    "    'question': 'Who went to the store ?',\n",
    "    'context': 'Adam went to the store yesterday.'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GPT-2 text generation\n",
    "\n",
    "Causal language modeliing is when the $i^{th}$ token is modeled based on all the previous tokens as opposed to masked language modeling where both left and right context are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "text_generator = pipeline(\"text-generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(text_generator(\"This is a serious issue we should address\", max_length=50, do_sample=False)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "print(text_generator(\"Twitter is a bad idea, Jack Dorsey had a bad day when he came up with it\", max_length=100, do_sample=False)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "del text_generator\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Further information\n",
    "\n",
    "[Official PyTorch Transformer tutorial](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)\n",
    "\n",
    "[Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)\n",
    "- Famous blog post with a detailed gentle introduction to Transformers\n",
    "\n",
    "[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- A walkthrough of original Transformer paper with code and detailed illustration\n",
    "\n",
    "[Huggingface Transformers - Summary of tasks](https://huggingface.co/transformers/task_summary.html)\n",
    "\n",
    "[My blog post about mBERT's tokenizer](http://juditacs.github.io/2019/02/19/bert-tokenization-stats.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
